{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "import os\n",
    "from openpyxl import load_workbook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# File paths (update these with actual file paths, where the files are located on your system)\n",
    "\n",
    "donor_file_path = \"sample_donor_metadata.xlsx\" # Path to the filled out donor metadata file\n",
    "bulk_file_path = \"C:/Users/stouq/Downloads/bulk_import_template.xlsx\" # Path to the bulk import template file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract donor filename (without extension)\n",
    "donor_filename = os.path.splitext(os.path.basename(donor_file_path))[0]\n",
    "\n",
    "# Create output file name dynamically, so you know what each file is for or what\n",
    "# collections it references\n",
    "output_file_path = f\"bulk_file_filled_{donor_filename}.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bulk_file_filled_sample_donor_metadata.xlsx'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the donor metadata file\n",
    "donor_df = pd.read_excel(donor_file_path)\n",
    "\n",
    "# Load the bulk_import_template file\n",
    "bulk_df = pd.read_excel(bulk_file_path, header=4)\n",
    "\n",
    "# Make a copy of the bulk_import_template file so you don't overwrite the original\n",
    "shutil.copy(bulk_file_path, output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the General column by merging the separate fields (created_by, place, people, context) in the donor metadata file\n",
    "def format_general(row):\n",
    "    parts = []\n",
    "    if pd.notna(row.get(\"created_by\", None)):  # Use .get() to avoid KeyError\n",
    "        parts.append(f\"Created by {row['created_by']}.\")\n",
    "    if pd.notna(row.get(\"place\", None)):\n",
    "        parts.append(f\"Created in {row['place']}.\")\n",
    "    if pd.notna(row.get(\"people\", None)):\n",
    "        parts.append(f\"People described/pictured: {row['people']}.\")\n",
    "    if pd.notna(row.get(\"context\", None)):\n",
    "        parts.append(f\"{row['context']}.\")\n",
    "    if pd.notna(row.get(\"id_number\", None)):\n",
    "        parts.append(f\"ID Number: {row['id_number']}\")\n",
    "    \n",
    "    return \" \".join(parts) if parts else None  # Return None if all fields are empty\n",
    "\n",
    "# Apply the function to create the 'General' column for the bulk_import_template file\n",
    "donor_df[\"General\"] = donor_df.apply(format_general, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define column mapping from Donor File to Bulk File\n",
    "column_mapping = {\n",
    "    \"grouping_or_item\": \"Description Level\",\n",
    "    \"title\": \"Title\",\n",
    "    \"date_begin\": \"Date(1) Begin\",\n",
    "    \"date_end\": \"Date(1) end\",\n",
    "    \"quantity\": \"Extent number\",\n",
    "    \"measurement\": \"Extent type: cubic feet, cds, etc.\",\n",
    "    \"history_of_ownership\": \"Custodial History\",\n",
    "    \"General\": \"General\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\stouq\\AppData\\Local\\Temp\\ipykernel_38096\\709559630.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  updated_data = pd.concat([existing_data, donor_df], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully transferred. The updated file is saved as: bulk_file_filled_sample_donor_metadata.xlsx\n"
     ]
    }
   ],
   "source": [
    "donor_df = donor_df.rename(columns=column_mapping)\n",
    "\n",
    "# Transform grouping_or_item values (grouping and item) \n",
    "# to match the bulk import template values (series and file)\n",
    "donor_df[\"Description Level\"] = donor_df[\"Description Level\"].replace({\n",
    "    \"grouping\": \"series\",\n",
    "    \"item\": \"file\"\n",
    "})\n",
    "\n",
    "# Use the Hierarchical Relationship logic to determine the level of the description\n",
    "donor_df[\"Hierarchical Relationship\"] = donor_df[\"Description Level\"].map({\n",
    "    \"series\": 1,\n",
    "    \"file\": 2\n",
    "})\n",
    "\n",
    "# Add Date Type logic based on Date Begin and Date End\n",
    "def determine_date_type(row):\n",
    "    if pd.notna(row[\"Date(1) Begin\"]) and pd.isna(row[\"Date(1) end\"]):\n",
    "        return \"single\"\n",
    "    elif pd.notna(row[\"Date(1) Begin\"]) and pd.notna(row[\"Date(1) end\"]):\n",
    "        return \"inclusive\"\n",
    "    return None  # Return None for rows without a date begin or date end\n",
    "\n",
    "donor_df[\"Date(1) Type\"] = donor_df.apply(determine_date_type, axis=1)\n",
    "\n",
    "# Load the workbook and get the active sheet to preserve formatting\n",
    "wb = load_workbook(output_file_path)\n",
    "ws = wb.active\n",
    "\n",
    "# Load existing data starting from row 6 (preserving rows 1-5), since headers start on row 5\n",
    "existing_data = pd.read_excel(output_file_path, header=4)\n",
    "\n",
    "donor_df = donor_df[[col for col in bulk_df.columns if col in donor_df.columns]]\n",
    "\n",
    "# Append new data below the existing data (so you cant overwrite any existing data \n",
    "# and can run multiple donor metadata files through at once)\n",
    "updated_data = pd.concat([existing_data, donor_df], ignore_index=True)\n",
    "\n",
    "# Write updated data starting from row 6 (after the header)\n",
    "for r_idx, row in enumerate(updated_data.itertuples(), 6):  # Start writing from row 6\n",
    "    for c_idx, value in enumerate(row[1:], 1):  # Skip the index (row[0])\n",
    "        ws.cell(row=r_idx, column=c_idx, value=value)\n",
    "\n",
    "# Save the updated workbook (with the preserved formatting)\n",
    "wb.save(output_file_path)\n",
    "\n",
    "print(f\"Data successfully transferred. The updated file is saved as: {output_file_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
